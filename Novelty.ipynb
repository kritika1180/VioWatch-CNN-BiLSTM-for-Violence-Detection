{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPVzOLd6UF8O"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Attention\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)#learning rate scheduling\n",
        "\n",
        "inputs = Input(shape=[10, IMG_SIZE, IMG_SIZE, 3], name=\"input\")\n",
        "convrnn = Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "convrnn = BatchNormalization()(convrnn)\n",
        "convrnn = Activation('relu')(convrnn)\n",
        "convrnn = Conv2D(64, (3, 3))(convrnn)\n",
        "convrnn = BatchNormalization()(convrnn)\n",
        "convrnn = Activation('relu')(convrnn)\n",
        "convrnn = MaxPooling2D((2, 2))(convrnn)\n",
        "convrnn = Conv2D(64, (3, 3))(convrnn)\n",
        "convrnn = BatchNormalization()(convrnn)\n",
        "convrnn = Activation('relu')(convrnn)\n",
        "convrnn = MaxPooling2D((2, 2))(convrnn)\n",
        "convrnn = Conv2D(64, (3, 3))(convrnn)\n",
        "convrnn = BatchNormalization()(convrnn)\n",
        "convrnn = Activation('relu')(convrnn)\n",
        "convrnn = MaxPooling2D((2, 2))(convrnn)\n",
        "convrnn = Flatten()(convrnn)\n",
        "\n",
        "convrnn = tf.reshape(convrnn, (1, 10, 6400))\n",
        "\n",
        "lstm_fw = LSTM(units=32, return_sequences=True)\n",
        "lstm_bw = LSTM(units=32, go_backwards=True, return_sequences=True)\n",
        "bi_lstm = Bidirectional(lstm_fw, backward_layer=lstm_bw)(convrnn)\n",
        "\n",
        "\n",
        "attention = Attention()([bi_lstm, bi_lstm])#attention mechanism\n",
        "\n",
        "convrnn = Dense(64)(attention)\n",
        "convrnn = BatchNormalization()(convrnn)\n",
        "convrnn = Activation('relu')(convrnn)\n",
        "convrnn = Dense(32)(convrnn)\n",
        "convrnn = BatchNormalization()(convrnn)\n",
        "convrnn = Activation('relu')(convrnn)\n",
        "output = Dense(1, activation='sigmoid')(convrnn)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), callbacks=[reduce_lr])\n"
      ]
    }
  ]
}